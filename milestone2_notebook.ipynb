{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We are going to analyze the dataset, cleanliness, blabla, start to explore the question.\n",
    "\n",
    "ntm lucas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview\n",
    "\n",
    "Size of dataset, blablabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T17:20:32.099752Z",
     "start_time": "2019-11-24T17:20:27.653244Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_FOOD_FACTS_PATH = 'data/en.openfoodfacts.org.products.csv'\n",
    "food_facts_df = pd.read_csv(OPEN_FOOD_FACTS_PATH, delimiter='\\t',low_memory=False)\n",
    "food_facts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available fields\n",
    "\n",
    "The dataset description is available [here](https://static.openfoodfacts.org/data/data-fields.txt).\n",
    "\n",
    "\n",
    "\n",
    "This dataset is provided with a text file describing the different fields as follows :\n",
    "\n",
    "* fields that end with _t are dates in the UNIX timestamp format (number of seconds since Jan 1st 1970)\n",
    "* fields that end with _datetime are dates in the iso8601 format: yyyy-mm-ddThh:mn:ssZ\n",
    "* fields that end with _tags are comma separated list of tags (e.g. categories_tags is the set of normalized tags computer from the categories field)\n",
    "* fields that end with a language 2 letter code (e.g. fr for French) is the set of tags in that language\n",
    "* fields that end with _100g correspond to the amount of a nutriment (in g, or kJ for energy) for 100 g or 100 ml of product\n",
    "* fields that end with _serving correspond to the amount of a nutriment (in g, or kJ for energy) for 1 serving of the product\n",
    "\n",
    "**TODO**: show data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General cleaning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each product of the dataset has a field named `code`. This should be unique to each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"We have {} rows in the dataset\".format(food_facts_df.shape[0]))\n",
    "print(\"We have {} different code in the dataset\".format((np.unique(food_facts_df['code']).shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the code is unique for each row of the dataset except for a small number of duplicates. Let's check that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_duplicates=food_facts_df['code'].duplicated()\n",
    "print(\"We have {} duplicated code\".format(number_duplicates.value_counts()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by dropping these duplicated rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "food_facts_df.drop_duplicates(subset=['code'],inplace=True)\n",
    "print(\"We have {} rows in the dataset after dropping the duplicates.\".format(food_facts_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We have now a unique code for each row. We can set the code as an index for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets fix the code as indes \n",
    "food_facts_df.set_index('code', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the 5 first rows in the dataset, the data is messy and has a huge number of NaNs. We need a huge data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data completeness\n",
    "\n",
    "Sparse dataset, lots of quasi empty columns (Lucas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creators of each row of the dataset are provided as a field. Let's try to see who are the main contributors to this data before starting to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computing the number of contributors\n",
    "nb_contributors=food_facts_df['creator'].unique().shape[0]\n",
    "print(\"We have overall {} contributors\".format(nb_contributors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many contributions each creator has in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the contributions \n",
    "counts = food_facts_df['creator'].value_counts()\n",
    "counts = counts.to_frame(name='counts')\n",
    "counts['percentage'] = counts['counts']/food_facts_df.shape[0]\n",
    "\n",
    "# Extracting only the top find and calculating the percentage of contribution\n",
    "top_10 = round(sum(counts[0:10]['percentage']),2)*100\n",
    "print(\"The 10 main\", 'creators',\"account for\", top_10,\"% of all contributions \\n\")\n",
    "print(counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After googling these 10 main contributors we can easily identify them: \n",
    "* `kiliweb` represents the YUKA app (a French app aiming to compare the products based on different criterions).\n",
    "* `usda` represnets United States Department of Agriculture. \n",
    "* `openfoodfacts-contributors` a bench of 20 main contributors.\n",
    "* `data-limite-app` food application available for ios and androids users (French app).\n",
    "* `Elcoco` also a food application (spanish app).\n",
    "* `openfood-ch-import` seems to be openfood contributors from Switzerland.\n",
    "* `sebleouf`, `tacite` , `tacinte` and `waistline-app` are unknown contributors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 6 sources of contributions seem to be reliable since we can clearly understand where the data comes from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the top 6\n",
    "top5_cont=counts.head(6).index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first 6 editors have 85% of the total contributions and are reliable. We will base our analysis on their contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_df=food_facts_df.loc[food_facts_df['creator'].isin([top5_cont[0],top5_cont[1],top5_cont[2],top5_cont[3],top5_cont[4],top5_cont[5]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('There are {:,} rows '.format(food_facts_df.shape[0]) + 'and {} columns in our data'.format(food_facts_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: How do nutrients influence the selected nutrition scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of the dataser fields are nutrients. We will try in this part to see their impact on the nutrition score. This analysis requires an additional cleaning of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will see the data type of each field: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "food_facts_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discard all no-float fields since they cannot represent the nutrients. (ie:object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_df = food_facts_df.select_dtypes(include=[np.float])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_facts_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('There are {:,} rows '.format(food_facts_df.shape[0]) + 'and {} columns left in our data'.format(food_facts_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now analyze the level of completness for each editor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "food_facts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our dataset has a huge number of NaNs. Dropping all of them will lead to an empty dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing values\n",
    "As an efficient strategy to deal with these missing values, we will first count the number of NaNs from the dataset and then remove the features where the number of missing values exceeds a certain threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count the number of nans for each feature: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_df=food_facts_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaNs_distribution_series=nutrition_facts_df.isnull().sum().sort_values()\n",
    "NaNs_distribution_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the distribution of NaNs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_df.isnull().sum().plot(kind='hist', figsize=(12,5),bins=50)\n",
    "plt.title('Distribution of NaNs')\n",
    "plt.xlabel('NaNs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many columns have a huge number of missing values(More than 85 columns with 350000 NaNs), let's try to remove some of them by playing with the threshold value.   \n",
    "We will plot the new distribution of NaNs. Our main objective is to remove this huge peak of frequency visualized in the plot above.   \n",
    "We tune the threshold with a trial and error to have a satisfactory result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_thresh_df = nutrition_facts_df.dropna(axis=1, thresh= len(nutrition_facts_df)*0.2,how='all')\n",
    "nutrition_facts_thresh_df = nutrition_facts_thresh_df.dropna(axis=0, how='all')\n",
    "\n",
    "print('There are now {:,} rows '.format(nutrition_facts_thresh_df.shape[0]) + 'and {} columns left in our data'.format(nutrition_facts_thresh_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_thresh_df.isnull().sum().plot(kind='hist', figsize=(10,5),bins=50)\n",
    "plt.title('Distribution of NaNs')\n",
    "plt.xlabel('NaNs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will se the total number of missing values for each field in the dataset after filtering them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_thresh_df.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"We have now {} of samples with {} fields\".format(nutrition_facts_thresh_df.shape[0],\n",
    "                                                         nutrition_facts_thresh_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We think that this is the optimal way to decrease the number of NaNs in this dataset. Putting the threshold higher then the selected value will delete the nutrition score columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dealing with the missing values, let's check the statistics of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the stats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nutrition_facts_thresh_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, It seems that there is no difference between the `nutrition-score-fr_100g` and `nutrition-score-uk_100g`. Let's analyse that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_thresh_df[['nutrition-score-fr_100g','nutrition-score-uk_100g']].select_dtypes(include=float).plot(kind='box', subplots=True, title='Nutrition score uk and fr boxplots', figsize=(8,5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the two scores has the same distribution except some outliers in the `nutrition-score-fr_100g`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with outliers\n",
    "\n",
    "Visualizing the stats. We see a huge number of outliers in the dataset after looking to min, max, mean and standard deviation of each feature.\n",
    "All the ingredient features should have a reasonable value <=100g except the energy which is expressed in kJ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the column `serving_quantity` is not an ingredient. It can be dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_thresh_df=nutrition_facts_thresh_df.drop(columns=['serving_quantity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the columns fields in a list\n",
    "fields=list(nutrition_facts_thresh_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now remove the outliers. All nutrition feautres should have a value <=100g (except energy_100g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_clean_df= nutrition_facts_thresh_df[(nutrition_facts_thresh_df\n",
    "                                                      [nutrition_facts_thresh_df.columns.difference(['energy_100g'])] <= 100.0).all(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative values can not be possible. Except for the nutrition score that is by definition defined between -14 and 40 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nutrition_facts_clean_df = nutrition_facts_clean_df[(nutrition_facts_clean_df[nutrition_facts_clean_df.columns.difference(['nutrition_score_fr_100g', 'nutrition_score_uk_100g'])] >= 0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"We have only {} rows left\".format(nutrition_facts_clean_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost a huge number of rows by removing the outliers. The dataset is highly corrupted in terms of ingredients.  We are going to see later if this number of samples is enough to have significant results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nutrition_facts_clean_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lost a huge number of rows but the dataset is now very clean !! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a clean dataset. Our strategy to find the impact of each variable on the nutrition score is the following: \n",
    "\n",
    "* 1) Correlation analysis between the variables \n",
    "* 2) Performing ordinary least square (OLS) regression using statsmodel \n",
    "* 3) Coefficient analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before analyzing the impact of our variables on the nutrition score, we need to normalize our data by removing the mean and dividng by the standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_clean_df = nutrition_facts_clean_df.select_dtypes(include=float).transform(lambda x: (x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check now if the mean of our variables is close to zero and the standard deviation is close to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_facts_clean_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! our data is now ready for the analysis on the impact of our variables on the nutrition score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing so, let's see the correlation between variables visualized in a heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_variables = nutrition_facts_clean_df.corr(method = \"pearson\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.heatmap(correlation_variables, mask=np.zeros_like(correlation_variables, dtype=np.bool),cmap=sns.diverging_palette(200, 10, as_cmap=True), square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot of correlation above we can confirm first that `nutrition-score-fr_100g` and `nutrition-score-uk_100g` are nearly the same.    \n",
    "We can see also a  correlation equal to 1 between `sodium_100g` and `salt_100g`. This is why we will drop the column `sodium_100g` to avoid redundancy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop sodium column. \n",
    "nutrition_facts_clean_df=nutrition_facts_clean_df.drop(columns=['sodium_100g'])\n",
    "fields_left=nutrition_facts_clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop one of the nutrition scores \n",
    "nutrition_facts_clean_df=nutrition_facts_clean_df.drop(columns=['nutrition-score-uk_100g'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_left=nutrition_facts_clean_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the heatmap again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating pearson correlation\n",
    "correlation_variables = nutrition_facts_clean_df.corr(method = \"pearson\")\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "#we draw the heatmap using seaborn\n",
    "sns.heatmap(correlation_variables, mask=np.zeros_like(correlation_variables, dtype=np.bool),cmap=sns.diverging_palette(200, 10, as_cmap=True), square=True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the heatmap, we can already see that `energey_100g`, `fat_100g`, `saturated_fat_100g` and `sugar_100g` have the highest correlation on the nutrition score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now everything is ready. What we want to do is to find the impact of each ingredient on the nutrition score. One way to do that is a regression. It allows us to see the impact of each variable on this score by looking to the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our formula for the regression: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the columns by replacing '-' by '_'. This will allow us to create the formula for the statsmodel regression since'-' is interpreted as a math operation while definiing the formula as a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in fields: \n",
    "    nutrition_facts_clean_df = nutrition_facts_clean_df.rename(columns={field: field.replace('-','_')})\n",
    "fields=nutrition_facts_clean_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nutrition_facts_clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS regression formula \n",
    "fields_left=nutrition_facts_clean_df.columns\n",
    "formula_string=\"nutrition_score_fr_100g ~ \"\n",
    "for field in fields_left[:-1]:\n",
    "    formula_string=formula_string+field+\"+\"\n",
    "formula_string=formula_string[:-1]\n",
    "formula_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nutrition_score_regression = sm.ols(formula= formula_string, data = nutrition_facts_clean_df)\n",
    "res = nutrition_score_regression.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysing the regression results from the coeficents of each ingredient, we can deduce the impact of these ingredients on the the nutrition score.  \n",
    "\n",
    "#### Positive coef \n",
    "* `sugars_100g` has the highest positive impact nutrition score ~ 0.37  \n",
    "* `saturated_fat_100g`has the fourth highest impact on nutrition socre ~ 0,36\n",
    "* `energy_100` has the second highest nutrition score ~ 0.18 \n",
    "* `fat_100g` with a coef ~ 0.16\n",
    "* `salt_100g` with a coef ~ 0.16\n",
    "* `nova_group`with a coef ~ 0.12\n",
    "\n",
    "\n",
    "#### Negative coef \n",
    "* `fiber_100g` has the highest negative nutrition score ~-0.2 \n",
    "\n",
    "#### Zero coef\n",
    "* `ingredients_from_palm_oil_n ` , `additives_n  ` and `carbohydrates_100g` don't have any impact on the nutrition score \n",
    "\n",
    "From the french nutrition score datasheet page 24, we see that the highest the nutrition score is the lowest the nutritional quality is. We can then deduce that all products with a high percentage of energy, sugar, salt, and saturated fat all unhealthy and products with a high percentage of fiber are healthy. All other ingredients do not have a huge impact on the nutrition score.  \n",
    "By analyzing the p-value we can deduce that our variables are statistically significant except the `ingredients_that_may_be_from_palm_oil_n`.    \n",
    "The number of samples we have in hand after dropping the outliers and NaNs is enough to have significant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the features which seems to have a high impact on the nutrition score is the nova group. The values of this group are {1,2,3,4}\n",
    "\n",
    "* Group 1 - Unprocessed or minimally processed foods :Unprocessed (or natural) foods are edible parts of plants (seeds, fruits, leaves, stems, roots) or of animals (muscle, offal, eggs, milk), and also fungi, algae and water, after separation from nature.\n",
    "* Group 2 - Processed culinary ingredients : Processed culinary ingredients, such as oils, butter, sugar and salt, are substances derived from Group 1 foods or from nature by processes that include pressing, refining, grinding, milling and drying.\n",
    "* Group 3 - Processed foods : Processed foods, such as bottled vegetables, canned fish, fruits in syrup, cheeses and freshly made breads, are made essentially by adding salt, oil, sugar or other substances from Group 2 to Group 1 foods. \n",
    "* Group 4 - Ultra-processed food and drink products: Ultra-processed foods, such as soft drinks, sweet or savoury packaged snacks, reconstituted meat products and pre-prepared frozen dishes.  \n",
    "\n",
    "Beeing in a higher group leads also to a high nutrition score and then low nutritional quality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the analysis of the nutrients on the nutrition score is giving a satisfactory results. The main problem we encontered here is the missing values. We were forced to loose a huge number of rows and fetures from the dataset.  \n",
    "We think that maybe some products don't contain some of the ingredients so the editor didn't  set the corresponding value to zero and thus added a missing value to the dataset. This can explain the huge number of missing values in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Comparing bio vs. normal products, is there a real difference in composition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "quick conclusion, we keep the question, further investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: What are the levels of interdependencies between nations based on their production and importations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "quick conclusion, we **don't keep** the question, further investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: proposed data story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
